{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f613096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>From</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>언니 동생으로 부르는게 맞는 일인가요..??</td>\n",
       "      <td>불안</td>\n",
       "      <td>단발성 대화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그냥 내 느낌일뿐겠지?</td>\n",
       "      <td>불안</td>\n",
       "      <td>단발성 대화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아직너무초기라서 그런거죠?</td>\n",
       "      <td>불안</td>\n",
       "      <td>단발성 대화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유치원버스 사고 낫다던데</td>\n",
       "      <td>불안</td>\n",
       "      <td>단발성 대화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>근데 원래이런거맞나요</td>\n",
       "      <td>불안</td>\n",
       "      <td>단발성 대화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171048</th>\n",
       "      <td>부동산 임대 소득으로 현재 여유롭게 살 수 있어서 좋단다.</td>\n",
       "      <td>행복</td>\n",
       "      <td>감성대화말뭉치_val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171049</th>\n",
       "      <td>폐결핵은 이미 완치된 것 같은데 약을 한 달이나 더 먹으라고 하네? 아직 안 나은 ...</td>\n",
       "      <td>불안</td>\n",
       "      <td>감성대화말뭉치_val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171050</th>\n",
       "      <td>연애하고 싶은데 소개팅만 나가면 꽝이야. 이러다가 난 결혼 못 하고 늙어 죽을 거야.</td>\n",
       "      <td>분노</td>\n",
       "      <td>감성대화말뭉치_val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171051</th>\n",
       "      <td>은행 대출이 막혀서 생활비를 구할 수가 없어. 이제 어떻게 살아야 하나 막막해.</td>\n",
       "      <td>분노</td>\n",
       "      <td>감성대화말뭉치_val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171052</th>\n",
       "      <td>자식들은 키워 준 은혜도 모르고 내게 오지도 않네. 너무 외롭고 슬퍼.</td>\n",
       "      <td>분노</td>\n",
       "      <td>감성대화말뭉치_val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171053 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sentence Emotion         From\n",
       "0                                언니 동생으로 부르는게 맞는 일인가요..??      불안       단발성 대화\n",
       "1                                            그냥 내 느낌일뿐겠지?      불안       단발성 대화\n",
       "2                                          아직너무초기라서 그런거죠?      불안       단발성 대화\n",
       "3                                           유치원버스 사고 낫다던데      불안       단발성 대화\n",
       "4                                             근데 원래이런거맞나요      불안       단발성 대화\n",
       "...                                                   ...     ...          ...\n",
       "171048                   부동산 임대 소득으로 현재 여유롭게 살 수 있어서 좋단다.      행복  감성대화말뭉치_val\n",
       "171049  폐결핵은 이미 완치된 것 같은데 약을 한 달이나 더 먹으라고 하네? 아직 안 나은 ...      불안  감성대화말뭉치_val\n",
       "171050    연애하고 싶은데 소개팅만 나가면 꽝이야. 이러다가 난 결혼 못 하고 늙어 죽을 거야.      분노  감성대화말뭉치_val\n",
       "171051       은행 대출이 막혀서 생활비를 구할 수가 없어. 이제 어떻게 살아야 하나 막막해.      분노  감성대화말뭉치_val\n",
       "171052            자식들은 키워 준 은혜도 모르고 내게 오지도 않네. 너무 외롭고 슬퍼.      분노  감성대화말뭉치_val\n",
       "\n",
       "[171053 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./data/sentiment_data_plus.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b5e47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>From</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>언니 동생으로 부르는게 맞는 일인가요..??</td>\n",
       "      <td>불안</td>\n",
       "      <td>단발성 대화</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그냥 내 느낌일뿐겠지?</td>\n",
       "      <td>불안</td>\n",
       "      <td>단발성 대화</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아직너무초기라서 그런거죠?</td>\n",
       "      <td>불안</td>\n",
       "      <td>단발성 대화</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유치원버스 사고 낫다던데</td>\n",
       "      <td>불안</td>\n",
       "      <td>단발성 대화</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>근데 원래이런거맞나요</td>\n",
       "      <td>불안</td>\n",
       "      <td>단발성 대화</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Sentence Emotion    From  encoded_label\n",
       "0  언니 동생으로 부르는게 맞는 일인가요..??      불안  단발성 대화              2\n",
       "1              그냥 내 느낌일뿐겠지?      불안  단발성 대화              2\n",
       "2            아직너무초기라서 그런거죠?      불안  단발성 대화              2\n",
       "3             유치원버스 사고 낫다던데      불안  단발성 대화              2\n",
       "4               근데 원래이런거맞나요      불안  단발성 대화              2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(data['Emotion'])\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "data['encoded_label'] = np.asarray(label_encoder.transform(data['Emotion']), dtype=np.int32)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "926537d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig, XLNetTokenizer\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "config = BertConfig.from_pretrained('skt/kobert-base-v1')\n",
    "config.num_labels = num_labels\n",
    "model = BertForSequenceClassification.from_pretrained('skt/kobert-base-v1', config=config)\n",
    "\n",
    "model.to('cuda')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc861fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46bb1358",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = data['encoded_label']\n",
    "dataset = data.loc[:, ['Sentence', 'encoded_label']]\n",
    "\n",
    "trainset, test_val_set = train_test_split(dataset,\n",
    "                                    test_size=0.2,\n",
    "                                    shuffle=True,\n",
    "                                    random_state=42,\n",
    "                                    stratify=target)\n",
    "\n",
    "# X_train = X_train.reset_index(drop=True)\n",
    "# X_test_val = X_test_val.reset_index(drop=True)\n",
    "# y_train = y_train.reset_index(drop=True)\n",
    "# y_test_val = y_test_val.reset_index(drop=True)\n",
    "\n",
    "t_target = test_val_set['encoded_label']\n",
    "testset, validset = train_test_split(test_val_set,\n",
    "                                   test_size=0.5,\n",
    "                                   shuffle=True,\n",
    "                                   random_state=42,\n",
    "                                   stratify=t_target)\n",
    "\n",
    "trainset = trainset.reset_index(drop=True)\n",
    "testset = testset.reset_index(drop=True)\n",
    "validset = validset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "564a75e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 3803, 5573, 517, 6783, 6705, 5782, 54, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"일기를 씁시다.\", truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aafe36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 630, 3], [2, 517, 380, 3], [2, 517, 435, 3], [2, 632, 3], [2, 3803, 3], [2, 1258, 3], [2, 517, 6116, 3], [2, 3], [2, 517, 6783, 3], [2, 2959, 3], [2, 1562, 3]], 'token_type_ids': [[0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0, 0], [0, 0, 0, 0], [0, 0, 0], [0, 0, 0]], 'attention_mask': [[1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus(\"<br>일기를 씁시다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e776e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 3135, 5724, 7814, 3], [2, 2207, 5345, 6701, 54, 3], [2, 3803, 5561, 3084, 5760, 910, 517, 7313, 5397, 5782, 54, 1205, 6150, 4164, 1174, 7396, 6797, 54, 3]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"안녕하세요\", \"반갑습니다.\", \"일기 쓰는 것은 즐겁다. 그렇지만 조금 귀찮아.\"]\n",
    "\n",
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "661401af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 3135, 5724, 7814, 3], [2, 2207, 5345, 6701, 54, 3], [2, 3803, 5561, 3084, 5760, 910, 517, 7313, 5397, 5782, 54, 1205, 6150, 4164, 1174, 7396, 6797, 54, 3]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1cc1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_encodings = tokenizer.batch_encode_plus(trainset['Sentence'])\n",
    "# val_encodings = tokenizer.batch_encode_plus(validset['Sentence'])\n",
    "# test_encodings = tokenizer.batch_encode_plus(testset['Sentence'])\n",
    "\n",
    "train_encodings = tokenizer(trainset['Sentence'].tolist(), truncation=True, padding=True, max_length=64, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(validset['Sentence'].tolist(), truncation=True, padding=True, max_length=64, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(testset['Sentence'].tolist(), truncation=True, padding=True, max_length=64, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "214c14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = Dataset(train_encodings, trainset['encoded_label'].tolist())\n",
    "val_dataset = Dataset(val_encodings, validset['encoded_label'].tolist())\n",
    "test_dataset = Dataset(test_encodings, testset['encoded_label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e6b2e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9f4a502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elice/anaconda3/envs/nightsky/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 136842\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6417\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6417' max='6417' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6417/6417 1:13:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.934300</td>\n",
       "      <td>0.892040</td>\n",
       "      <td>0.681983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.750300</td>\n",
       "      <td>0.870590</td>\n",
       "      <td>0.690810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.584300</td>\n",
       "      <td>0.922618</td>\n",
       "      <td>0.686309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_trainer/checkpoint-500\n",
      "Configuration saved in test_trainer/checkpoint-500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer/checkpoint-1000\n",
      "Configuration saved in test_trainer/checkpoint-1000/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer/checkpoint-1500\n",
      "Configuration saved in test_trainer/checkpoint-1500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer/checkpoint-2000\n",
      "Configuration saved in test_trainer/checkpoint-2000/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17106\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2500\n",
      "Configuration saved in test_trainer/checkpoint-2500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer/checkpoint-3000\n",
      "Configuration saved in test_trainer/checkpoint-3000/config.json\n",
      "Model weights saved in test_trainer/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer/checkpoint-3500\n",
      "Configuration saved in test_trainer/checkpoint-3500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer/checkpoint-4000\n",
      "Configuration saved in test_trainer/checkpoint-4000/config.json\n",
      "Model weights saved in test_trainer/checkpoint-4000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17106\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-4500\n",
      "Configuration saved in test_trainer/checkpoint-4500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer/checkpoint-5000\n",
      "Configuration saved in test_trainer/checkpoint-5000/config.json\n",
      "Model weights saved in test_trainer/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer/checkpoint-5500\n",
      "Configuration saved in test_trainer/checkpoint-5500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer/checkpoint-6000\n",
      "Configuration saved in test_trainer/checkpoint-6000/config.json\n",
      "Model weights saved in test_trainer/checkpoint-6000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17106\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6417, training_loss=0.7364725044848522, metrics={'train_runtime': 4383.3795, 'train_samples_per_second': 93.655, 'train_steps_per_second': 1.464, 'total_flos': 1.35023472725184e+16, 'train_loss': 0.7364725044848522, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=val_dataset,\n",
    "                  compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de6f8259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 17105\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2139' max='2139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2139/2139 01:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 0.06831191,  0.50074655, -0.4283551 , ..., -1.6985977 ,\n",
       "        -2.2770462 , -1.7191817 ],\n",
       "       [ 0.29235607,  2.9659004 , -0.33061096, ..., -1.2987227 ,\n",
       "        -3.127798  ,  3.971012  ],\n",
       "       [-2.850855  ,  2.7823305 , -2.2580345 , ...,  2.437744  ,\n",
       "        -2.1444926 ,  1.0928146 ],\n",
       "       ...,\n",
       "       [ 0.17570493, -1.2033936 , -3.464555  , ...,  4.362704  ,\n",
       "        -0.7634575 , -2.957487  ],\n",
       "       [-1.1810024 ,  4.3056917 , -2.264956  , ...,  1.4193431 ,\n",
       "        -2.2924385 , -0.29796773],\n",
       "       [ 1.4102197 , -1.3454844 , -1.9526913 , ...,  5.075554  ,\n",
       "        -2.5126026 , -2.4439557 ]], dtype=float32), label_ids=array([3, 1, 4, ..., 4, 1, 4]), metrics={'test_loss': 0.9414632320404053, 'test_accuracy': 0.6842443729903537, 'test_runtime': 65.366, 'test_samples_per_second': 261.68, 'test_steps_per_second': 32.723})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b191b57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_trainer\n",
      "Configuration saved in test_trainer/config.json\n",
      "Model weights saved in test_trainer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('test_trainer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "affec614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/elice/nightsky/test_trainer/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"skt/kobert-base-v1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"kobert_version\": 1.0,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading weights file /home/elice/nightsky/test_trainer/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /home/elice/nightsky/test_trainer/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "test_model = BertForSequenceClassification.from_pretrained('/home/elice/nightsky/test_trainer/', local_files_only=True)\n",
    "test_model.to('cuda')\n",
    "\n",
    "def prediction(predict_sentence):\n",
    "  if type(predict_sentence) == str:\n",
    "      data = [predict_sentence]\n",
    "  else:\n",
    "      data = predict_sentence\n",
    "\n",
    "  inputs = tokenizer(data, return_tensors='pt')\n",
    "  inputs.to('cuda')\n",
    "\n",
    "  with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "  predicted_class_id = logits.argmax().item()\n",
    "  output = model.config.id2label[predicted_class_id]\n",
    "    \n",
    "  if output == 0:\n",
    "    return \"공포\"\n",
    "  elif output == 1:\n",
    "    return \"놀람\"\n",
    "  elif output == 2:\n",
    "    return \"분노\"\n",
    "  elif output == 3:\n",
    "    return \"슬픔\"\n",
    "  elif output == 4:\n",
    "    return \"중립\"\n",
    "  elif output == 5:\n",
    "    return \"행복\"\n",
    "  elif output == 6:\n",
    "    return \"혐오\"\n",
    "  return \"공포: {0}, 놀람: {1}, 분노: {2}, 슬픔: {3}, 중립: {4}, 행복: {5}, 혐오: {6}\".format(logits[0], logits[1], logits[2], logits[3], logits[4], logits[5], logits[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd78a07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LABEL_5'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction(\"새로운 모델 학습 코드를 완성했습니다~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54eca4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nightsky",
   "language": "python",
   "name": "nightsky"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
